{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Автокодировщики\n",
    "\n",
    "Объект описывается слишком большим количеством признаков? Хочется уменьшить их чилсло и не потерять важную информацию? Вам надоел PCA? Тогда автокодировщики идут к вам! \n",
    "\n",
    "## Почиташки \n",
    "\n",
    "* [Хорошая серия статей](https://habrahabr.ru/post/331382/) про автокодировщики и GAN. Немного подворовал код у них. \n",
    "* [Глава книги](http://www.deeplearningbook.org/contents/autoencoders.html) deep learning book, на которой основана серия сатей выше. \n",
    "* [Статья из блога про Keras](https://blog.keras.io/building-autoencoders-in-keras.html), на которой также основана статья выше (все воруют код у всех, просто ужас, блин). \n",
    "* [Неделя 4 курса Introduction to deep learning](https://www.coursera.org/learn/intro-to-deep-learning/home/week/4), с которой также взята часть материалов, и на которой я был ментором (возможно, что это оправдывает меня). Можно заодно [посмотреть тетрадку с этого курса.](https://github.com/hse-aml/intro-to-dl/blob/master/week4/Autoencoders-task.ipynb) Неожиданно, но она тоже с лицами...  Неожиданно, но код похож... Или даже сворован..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Про автокодировщики \n",
    "\n",
    "Эпоха больших данных даёт нам слишком много данных. Часто хочется, чтобы их было немного поменьше, но при этом они выражали бы всю информацию о великом процессе порождения данных более ёмко. Не потерять в знании, но сэкономить! Именно из такого благородного помысла рождается метод главных компонент. Его посыл очень прост: давайте посмотрим на наши данные, найдём в них те направления, в которых происходит самое сильное изменение и оставим только их. При этом, в плане информации мы бы не потеряли особо много. Именно так часто поступают в случае, когда размерность данных очень большая. Находят $d$ Направлений, разброс в которых покрывает $90\\%$ дисперсии, а остальное забывают. \n",
    "\n",
    "Например, если у нас есть выборка из карасей и щук и требуется научится отличать одних от ругих, то мы могли бы оставить только информацию о чешуйках, расположенных вдоль первой, более длинной компоненты. На качество нашего классификатора это бы повлияло слабо, но мы, при этом, избавились бы от лишней размерности, от оценки лишнего коэфициента и могли бы пустить большее число наблюдений на оценку меньшего числа параметров.  \n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/9/90/PCA_fish.png/256px-PCA_fish.png)\n",
    "\n",
    "Главная фишка метода главных компонент состоит в том, что он делает всю эту редукцию линейно. На самом деле все новые признаки будут представлять из себя линейные комбинации из старых. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идею такой редукции лишней информации можно обобщить до нелинейного случая и сделать прыжок к нейросетям, которые называются **автоэнокдерами.**  Автоэнкодеры учатся оставлять минимум информации так, чтобы по этому минимуму было возможно восстановить с очень высокой точностью исходную информацию. \n",
    "\n",
    "Автоэнкодеры — это нейронные сети прямого распространения, которые восстанавливают входной сигнал на выходе. Внутри у них имеется скрытый слой, который представляет собой код, описывающий модель. Автоэнкодеры конструируются таким образом, чтобы не иметь возможность точно скопировать вход на выходе. Обычно их ограничивают в размерности кода. \n",
    "\n",
    "![](https://hsto.org/web/cf6/228/613/cf6228613fdc4f8fb819cbd41bb677eb.png) \n",
    "\n",
    "Закодированную часть Входной сигнал восстанавливается с ошибками из-за потерь при кодировании, но, чтобы их минимизировать, сеть вынуждена учиться отбирать наиболее важные признаки.\n",
    "\n",
    "Новый пример! Пусть у нас есть куча рукописных цифр и мы хотели бы научить компьютер понимать где какая цифра нарисована. Каждая цифра это картинка из пикселей размер $28 \\times 28$. Всего у нас $28^2$ наблюдаемые переменные. Это слишком много! Явно не каждый пиксель несёт в себе информацию о том какая цифра нарисована на картинке. Большая часть пикселей бесполезна. \n",
    "\n",
    "Чтобы отобрать самые важные нелинейные комбинации из пикселей, мы можем сделать следующее: \n",
    "\n",
    "![](https://blog.keras.io/img/ae/autoencoder_schema.jpg)\n",
    "\n",
    "Нейросеть получает на вход цифру, пытается оставить 10 самых важных нейлинейных комбинаций из пикселей, а после по этим самым важным комбинациям, она пытается восстановить цифру назад с как можно большей точностью. Ограничение на то сколько самых важных нелинейных пикселей должно остаться, заставляет нашу сетку стараться отобрать исключительно всё самое важное, что можно найти на картинке. Метод главных компонент будет частным, самым простым случаем, такой нейронной сетки. \n",
    "\n",
    "Первая часть автокодировщика $g(x)$ называется **encoder**. Она пытается закодировать картинку. Вторая часть $f(h)$ называет **decoder**. Она пытается раскодировать картинку. Автоэккодер, изменяя $f$ и $g$ пытается выучить тождественную функцию \n",
    "\n",
    "$$x = f(g(x)),$$ \n",
    "\n",
    "минимизируя какой-то функционал ошибки \n",
    "\n",
    "$$L(x, f(g(x))).$$ \n",
    "\n",
    "При этом выучить тождественную выборку он не может, так как в сердцевине находится  **бутылочное горлышко** с довольно маленьким числом нейронов. В итоге сетка вынуждена отаравлять в сердцевину только те комбинации фичей, по которым легче всего восстановить входные данные, и таким образом отсеивает ненужную информацию. В горлышке в результате обучения оказываются наши новые нелинейные фичи. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Данные.\n",
    "\n",
    "Будем учить свой автокодировщик для человеческих лиц. Пользоваться будем lfw-датасетом:\n",
    "\n",
    "* http://vis-www.cs.umass.edu/lfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=0, resize=0.5, color=False,\n",
    "                              slice_=(slice(94, 190, None), slice(78, 174, None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfw_people.data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfw_people.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfw_people.images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfw_people.target_names[0] # имена людей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = lfw_people.images\n",
    "IMG_SHAPE = X.shape[1:]\n",
    "\n",
    "X = X.astype('float32') / 255.0\n",
    "X = X[...,None]\n",
    "X_train, X_test = train_test_split(X, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0,:,:,0], cmap='gray')\n",
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хочу ещё данных!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 8\n",
    "rows = 2\n",
    "fig = plt.figure(figsize=(2 * cols - 1, 2.5 * rows - 1))\n",
    "for i in range(cols):\n",
    "    for j in range(rows):\n",
    "        random_index = np.random.randint(0, len(X_train))\n",
    "        ax = fig.add_subplot(rows, cols, i * rows + j + 1)\n",
    "        ax.grid(False)\n",
    "        ax.axis('off')\n",
    "        ax.imshow(X_train[random_index,:,:,0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С этим можно уже учить модели. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Собираем автокодировщик для PCA \n",
    "\n",
    "Как мы уже выяснили, простейшим автокодировщиком является PCA. Один слой на входе, бутылочное горлышко с новыми фичами, один слой на выходе и PCA готов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.prod(input_shape) # пикселей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pca_autoencoder(img_shape, code_size=36):\n",
    "    # code_size это размерность кодированного представления (фичей на выходе)\n",
    "    \n",
    "    # Энкодер \n",
    "    \n",
    "    #############\n",
    "    # Ваш код \n",
    "    #############\n",
    "    \n",
    "    # Декодер \n",
    "    \n",
    "    #############\n",
    "    # Ваш код \n",
    "    #############\n",
    "    \n",
    "    return encoder,decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось только собрать модель из моделей. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сконструировали энкодер и декодер \n",
    "encoder, decoder = build_pca_autoencoder(input_shape, code_size=36)\n",
    "\n",
    "inp = L.Input(input_shape)  # Вход\n",
    "code = encoder(inp)         # Энкодер от входа \n",
    "reconstruction = decoder(code)  # Декодер от кода \n",
    "\n",
    "# Скрепили две наши сетки в одну общую модель\n",
    "autoencoder = Model(inp, reconstruction)\n",
    "\n",
    "# Собираем сеть с методом оптимизации и выбранной ошибкой \n",
    "autoencoder.compile('adam', 'mse') \n",
    "\n",
    "# Посмотрим на параметры \n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание что энкодер восстанавливает вход на выходе. У нас нет меток для обучения, а надо, чтобы ими были сами по себе исходные картинки. Немного подправим генератор данных, чтобы это правда было так. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Обучаем модель \n",
    "hist_1 = autoencoder.fit(x = X_train, y = X_train,  # Обратите внимание что энкодер восстанавливает вход на выходе \n",
    "                epochs = 40,                        # Никакие y тут не нужны. Это просто лэйблы... \n",
    "                batch_size = 256,\n",
    "                shuffle = True,\n",
    "                validation_data = [X_test, X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наш автокодировщик обучился. это довольно приятная новость. Попробуем отрисовать оригинальные изображения, их компактные представления и то как по этим представлениям изображения восстанавливаются. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для отрисовки ооигинальных цифр, их представления и восстановленного рисунка\n",
    "def visualize(img, encoder, decoder):\n",
    "    code = encoder.predict(img[None])[0]\n",
    "    reco = decoder.predict(code[None])[0]\n",
    "\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title(\"Original\")\n",
    "    plt.imshow(img[:,:,0], cmap='gray')\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.title(\"Code\")\n",
    "    plt.imshow(code.reshape([code.shape[-1]//2,-1]))\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.title(\"Reconstructed\")\n",
    "    plt.imshow(reco.clip(0,1)[:,:,0], cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = autoencoder.evaluate(X_test, X_test,verbose=0)\n",
    "print(\"Final MSE:\",score)\n",
    "\n",
    "for i in range(3):\n",
    "    img = X_test[i]\n",
    "    visualize(img,encoder,decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь по оставшимся 36 фичам (вместо оригинальных 784) можно строить классификаторы. Попробуйте на досуге заняться этим весёлым занятием, а мы движемся дальше. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Глубокий автоэнкодер \n",
    "\n",
    "Всегда можно лучше! Сконструируем глубокий автокодировщик для вычленения нелинейных фичей. \n",
    "\n",
    "![ ](https://pbs.twimg.com/media/CYggEo-VAAACg_n.png:small)\n",
    "\n",
    "* Обратите внимание, что при постройке автокодировщика не должно быть слоёв меньших, чем бутылочное горлышко (выход энкодера). \n",
    "* Также обратите внимание, что вполне уместно использовать свёртки и пулинг, но такой автокодировщик будет построен чуточку ниже. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deep_autoencoder(input_shape, code_size=36):\n",
    "    \n",
    "    #############\n",
    "    # Ваш код \n",
    "    #############\n",
    "    \n",
    "    return encoder,decoder\n",
    "\n",
    "# Сконструировали энкодер и декодер \n",
    "encoder,decoder = build_deep_autoencoder(input_shape, code_size=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединили всё вместе и задали процедуру оптимизации для сетки\n",
    "\n",
    "    #############\n",
    "    # Ваш код \n",
    "    #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем модель \n",
    "hist_2 = autoencoder.fit(x = X_train, y = X_train,  # Обратите внимание что энкодер восстанавливает вход на выходе \n",
    "                epochs = 40,                        # Никакие y тут не нужны. Это просто лэйблы... \n",
    "                batch_size = 256,\n",
    "                shuffle = True,\n",
    "                validation_data=[X_test, X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = autoencoder.evaluate(X_test, X_test,verbose=0)\n",
    "print(\"Final MSE:\",score)\n",
    "\n",
    "for i in range(3):\n",
    "    img = X_test[i]\n",
    "    visualize(img,encoder,decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Свёрточный автокодировщик \n",
    "\n",
    "Добавим свёрточные слои и немного макспулинга.  Часть, которая относится к энкодеру будет стандартной. Это просто свёрточные слои и пулинг. Кончается она обычным свёрточным слоем с `code_size` нейронов. В качестве функции активации берите `elu`. \n",
    "\n",
    "Попробуйте повторить свёртку и пулинг $2\\times2$ четыре раза с ядром размера $3 \\times 3$ и `padding='same'`. У слоёв поставьте число каналов: $32$, $64$, $128$, $256$.\n",
    "\n",
    "\n",
    "Для декодера мы будем использовать \n",
    "\n",
    "`L.Conv2DTranspose(filters=?, kernel_size=(3, 3), strides=2, activation='elu', padding='same')`\n",
    "\n",
    "и\n",
    "\n",
    "`UpSampling2D((2,2))`. \n",
    "\n",
    "Сначала попробуйте скомбинировать эти два варианта, а после вспомните объяснение про `Conv2DTranspose` с предыдущей пары :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv_autoencoder(input_shape, code_size=36):\n",
    "    \n",
    "    #############\n",
    "    # Ваш код \n",
    "    #############\n",
    "   \n",
    "    \n",
    "    return encoder,decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код, чтобы скрепит всё вместе\n",
    "\n",
    "    #############\n",
    "    # Ваш код \n",
    "    #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sgd = tf.keras.optimizers.Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "autoencoder.compile(optimizer='adam', loss='mse') # Собираем сеть с методом оптимизации и выбранной ошибкой \n",
    "\n",
    "# Посмотрим на параметры \n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модель будет обучаться ну оооочень долго... \n",
    "# Ошибка падает довольно быстро, добавлять свёртку было хорошей идеей\n",
    "hist_3 = autoencoder.fit(X_train, X_train,\n",
    "                        epochs=10,\n",
    "                        batch_size=256,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = autoencoder.evaluate(X_test, X_test,verbose=0)\n",
    "print(\"Final MSE:\",score)\n",
    "\n",
    "for i in range(3):\n",
    "    img = X_test[i]\n",
    "    visualize(img,encoder,decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Denoising автоэнкодер \n",
    "\n",
    "Автоэнкодеры можно обучить убирать шум из данных: для этого надо на вход подавать зашумленные данные и на выходе сравнивать с данными без шума. Создадим модель, которая будет зашумлять входное изображение, а после пытаться избавить его от шума. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавляем на картинку немного шума \n",
    "def apply_gaussian_noise(X,sigma=0.01):\n",
    "    return X + sigma*np.random.normal(loc=0.0, scale=1.0, size=X.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,4,1)\n",
    "plt.imshow(X_train[0].reshape(48, 48), cmap = 'gray')\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(apply_gaussian_noise(X_train[:1],sigma=0.01)[0].reshape(48, 48), cmap = 'gray')\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(apply_gaussian_noise(X_train[:1],sigma=0.1)[0].reshape(48, 48), cmap = 'gray')\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(apply_gaussian_noise(X_train[:1],sigma=0.5)[0].reshape(48, 48), cmap = 'gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зашумляем данные \n",
    "X_train_noisy = apply_gaussian_noise(X_train, sigma=0.1)\n",
    "X_test_noisy = apply_gaussian_noise(X_test, sigma=0.1)\n",
    "\n",
    "# Убираем все заскоки за отрезок [0;1]\n",
    "X_train_noisy = np.clip(X_train_noisy, 0., 1.)\n",
    "X_test_noisy = np.clip(X_test_noisy, 0., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем сеточку из предыдущего пункта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# размер горлышка имеет смысл взять побольше (почему?)\n",
    "encoder_noisy, decoder_noisy = build_deep_autoencoder(input_shape, code_size=1024)\n",
    "\n",
    "inp = L.Input(input_shape)\n",
    "code = encoder_noisy(inp)\n",
    "reconstruction = decoder_noisy(code)\n",
    "\n",
    "autoencoder_noisy = Model(inp, reconstruction)\n",
    "autoencoder_noisy.compile('adam', 'mse')\n",
    "\n",
    "autoencoder_noisy.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_noise = autoencoder_noisy.fit(X_train_noisy, X_train,\n",
    "                        epochs=10,\n",
    "                        batch_size=256,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(X_test_noisy, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = autoencoder_noisy.evaluate(X_test_noisy, X_test,verbose=0)\n",
    "print(\"Final MSE:\",score)\n",
    "\n",
    "for i in range(3):\n",
    "    img = X_test_noisy[i]\n",
    "    visualize(img,encoder,decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.  Поиск похожих изображений \n",
    "\n",
    "С помощью найденного горлышка можно искать похожие друг на друга изображения. Чтобы ускорить процесс поиска, мы будем использовать похожий на метод ближайших соседей, но более быстрый алгоритм под названием [Locality Sensitive Hashing forest.](http://scikit-learn.org/0.16/modules/generated/sklearn.neighbors.LSHForest.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = X_train\n",
    "codes = encoder.predict(images)\n",
    "\n",
    "# хорошая идея ставить по ходу архитекрутуры проверки на размерности :) \n",
    "assert len(codes) == len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LSHForest\n",
    "lshf = LSHForest(n_estimators=50).fit(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar(image, n_neighbors=5):\n",
    "    \n",
    "    code = encoder.predict(image[None])\n",
    "    \n",
    "    (distances,),(idx,) = lshf.kneighbors(code,n_neighbors=n_neighbors)\n",
    "    \n",
    "    return distances,images[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_similar(image):\n",
    "    \n",
    "    distances,neighbors = get_similar(image,n_neighbors=11)\n",
    "    \n",
    "    plt.figure(figsize=[8,6])\n",
    "    plt.subplot(3,4,1)\n",
    "    plt.imshow(image.reshape(48,48), cmap = 'gray')\n",
    "    plt.title(\"Original image\")\n",
    "    \n",
    "    for i in range(11):\n",
    "        plt.subplot(3,4,i+2)\n",
    "        plt.imshow(neighbors[i].reshape(48,48), cmap = 'gray')\n",
    "        plt.title(\"Dist=%.3f\"%distances[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_similar(X_test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_similar(X_test[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_similar(X_test[66])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.  Прогулка по пространству эмбедингов.\n",
    "\n",
    "Можно брать два эмбединга, искать между ними линейную комбинацию и пытаться прогуляться от одного к другому. Попробуйте вытащить из для двух заданных изображений `image1` и `image2` несколько линейных комбинаций эмбедингов и посмотреть как выглядит картинка с плавным переходом от одной картинки к другой. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_len = 10\n",
    "\n",
    "for _ in range(5):\n",
    "    image1,image2 = X_test[np.random.randint(0,len(X_test),size=2)]\n",
    "\n",
    "    code1, code2 = # ваш код для поиска эмбединга\n",
    "\n",
    "    plt.figure(figsize=[15,6])\n",
    "    for i,a in enumerate(np.linspace(0,1,num=path_len)):\n",
    "\n",
    "        # ваш код для расчёта эмбединга\n",
    "        # и его перевода в картинку\n",
    "\n",
    "        plt.subplot(1, path_len, i+1)\n",
    "        plt.imshow(output_image.reshape(48,48), cmap = 'gray')\n",
    "        plt.title(\"a=%.2f\"%a)        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
